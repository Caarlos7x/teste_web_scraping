### TESTES DE NIVELAMENTO  

## 1. TESTE DE WEB SCRAPING
Este teste deve ser realizado nas linguagens Python ou Java. E o cÃ³digo deverÃ¡ executar o/a:

- 1.1. Acesso ao site: https://www.gov.br/ans/pt-br/acesso-a-informacao/participacao-da-
sociedade/atualizacao-do-rol-de-procedimentos
- 1.2. Download dos Anexos I e II em formato PDF
- 1.3. CompactaÃ§Ã£o de todos os anexos em um Ãºnico arquivo (formatos ZIP, RAR, etc.).

## 2. TESTE DE TRANSFORMAÃ‡ÃƒO DE DADOS
Crie um cÃ³digo em Python ou Java que execute as seguintes tarefas:
- 2.1. Extraia os dados da tabela **Rol de Procedimentos e Eventos em SaÃºde** do **PDF do Anexo I** (todas as pÃ¡ginas)
- 2.2. Salve os dados em uma tabela estruturada, em formato **CSV**.
- 2.3. Compacte o CSV em um arquivo denominado `"Teste_{seu_nome}.zip"`.
- 2.4. Substitua as abreviaÃ§Ãµes das colunas **OD** e **AMB** pelas descriÃ§Ãµes completas:
  - **OD** â†’ *Seg. OdontolÃ³gica*
  - **AMB** â†’ *Seg. Ambulatorial*

---

## **ğŸ“Œ Como Rodar o Projeto**
### **âš™ï¸ PrÃ©-requisitos**
Antes de rodar o projeto, verifique se vocÃª tem instalado:
- **Python 3.9+** â†’ [Baixar Python](https://www.python.org/downloads/)
- **Git** (caso queira clonar do repositÃ³rio) â†’ [Baixar Git](https://git-scm.com/downloads)
- **Bibliotecas do Python** (serÃ£o instaladas com `pip`)

### **ğŸ“¥ Clonando o RepositÃ³rio**
Se ainda nÃ£o clonou o projeto, execute no terminal:
```bash
git clone git@github.com:Caarlos7x/teste_web_scraping.git
cd teste_web_scraping
```

## **ğŸ“¦ Instalando as DependÃªncias**
Dentro do projeto, instale todas as bibliotecas necessÃ¡rias:
```bash
pip install -r requirements.txt
```
- As bibliotecas instaladas incluem:
- requests â†’ Para fazer requisiÃ§Ãµes HTTP
- beautifulsoup4 â†’ Para web scraping
- pdfplumber â†’ Para extrair dados de PDFs
- pandas â†’ Para manipulaÃ§Ã£o de dados
- zipfile â†’ Para compactar os arquivos

## **ğŸš€ Executando o Projeto**
Para rodar o script principal e iniciar o processo de web scraping + transformaÃ§Ã£o de dados, execute:
```bash
python src/main.py
```

## **ğŸ“‚ SaÃ­da Esperada**
1. O script **baixa os PDFs do site oficial** e os salva em `downloads/`.

2. Os PDFs sÃ£o **compactados em um Ãºnico arquivo ZIP**.

3. O **Anexo I** Ã© processado e os dados extraÃ­dos sÃ£o:

    - Convertidos para CSV.

    - SubstituÃ­das as siglas OD e AMB.

4. O **CSV final Ã© compactado em** `Teste_Carlos_Augusto.zip` dentro da pasta `output/`.

## **ğŸ“Œ Estrutura do Projeto**
```bash
teste_web_scraping/
â”‚â”€â”€ downloads/                # PDFs baixados
â”‚â”€â”€ output/                   # CSVs e arquivos compactados
â”‚â”€â”€ src/                      # CÃ³digo-fonte do projeto
â”‚   â”œâ”€â”€ main.py               # Arquivo principal que executa todo o processo
â”‚   â”œâ”€â”€ web_scraper.py        # Script responsÃ¡vel por baixar os PDFs
â”‚   â”œâ”€â”€ extract_data.py       # Script que processa os dados do PDF e gera CSV
â”‚â”€â”€ requirements.txt          # DependÃªncias do projeto
â”‚â”€â”€ README.md                 # DocumentaÃ§Ã£o do projeto
```

## **ğŸ“Œ Autor**
ğŸ‘¤ Carlos Augusto
ğŸ“§ Contato: caarlos.frei@gmail.com